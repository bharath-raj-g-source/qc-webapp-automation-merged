{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f73337ea-0543-44e2-8190-f97096411977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Filtered DataFrame (15_Italian GP II) Head ---\n",
      "       Country       Date       Channel   Programme Type       Audience  \\\n",
      "13496   France 2023-09-02  Canal+ Sport  Qualifying Live         283167   \n",
      "13497   France 2023-09-03        Canal+        Race Live        1327000   \n",
      "13498  Germany 2023-09-02    SkySportF1  Qualifying Live  146982.522657   \n",
      "13499  Germany 2023-09-03    SkySportF1        Race Live  541214.534177   \n",
      "13500  Germany 2023-09-03    SkyTopEven        Race Live   93749.240511   \n",
      "\n",
      "             Grand Prix  \n",
      "13496  15_Italian GP II  \n",
      "13497  15_Italian GP II  \n",
      "13498  15_Italian GP II  \n",
      "13499  15_Italian GP II  \n",
      "13500  15_Italian GP II  \n",
      "\n",
      "--- Data Types (Dtypes) ---\n",
      "Country                   object\n",
      "Date              datetime64[ns]\n",
      "Channel                   object\n",
      "Programme Type            object\n",
      "Audience                  object\n",
      "Grand Prix                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file and sheet\n",
    "NEW_FILE_NAME = \"F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "NEW_FILE_PATH = f\"data/{NEW_FILE_NAME}\"\n",
    "TARGET_SHEET = \"DATA\"\n",
    "\n",
    "# Define the columns you want to keep\n",
    "REQUIRED_COLUMNS = ['Country', 'Date', 'Channel', 'Programme Type', 'Audience'] \n",
    "\n",
    "# Define the filter\n",
    "FILTER_VALUE = '15_Italian GP II'\n",
    "FILTER_COLUMN = 'Grand Prix'\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(\n",
    "        NEW_FILE_PATH,\n",
    "        sheet_name=TARGET_SHEET,\n",
    "        usecols=REQUIRED_COLUMNS + [FILTER_COLUMN] \n",
    "    )\n",
    "    \n",
    "    # â­ NEW: Convert the 'Date' column to the proper datetime dtype â­\n",
    "    # errors='coerce' is helpful: if it finds a value it can't convert, \n",
    "    # it turns it into NaT (Not a Time), allowing the rest to convert.\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce') \n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_italian_gp = df[df[FILTER_COLUMN] == FILTER_VALUE].copy()\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"\\n--- Filtered DataFrame ({FILTER_VALUE}) Head ---\")\n",
    "    print(df_italian_gp.head())\n",
    "    \n",
    "    # Check the new dtype\n",
    "    print(\"\\n--- Data Types (Dtypes) ---\")\n",
    "    print(df_italian_gp.dtypes)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during loading or conversion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2359e04f-778f-4aca-a2c8-ac00b5202d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Region     Market  Market ID Broadcaster  \\\n",
      "0  Central and South America  Argentina       74.0        ESPN   \n",
      "1  Central and South America  Argentina       74.0    Mediapro   \n",
      "2  Central and South America  Argentina       74.0    Mediapro   \n",
      "3  Central and South America  Argentina       74.0    Mediapro   \n",
      "4  Central and South America  Argentina       74.0    Mediapro   \n",
      "\n",
      "         TV-Channel  Channel ID Pay/Free TV Date (UTC/GMT)       Date  \\\n",
      "0        ESPN 2 ARG      1210.0         NaN     2025-07-04 2025-07-03   \n",
      "1  Fox Sports (ARG)      2732.0         NaN     2025-07-04 2025-07-04   \n",
      "2  Fox Sports (ARG)      2732.0         NaN     2025-07-04 2025-07-04   \n",
      "3  Fox Sports (ARG)      2732.0         NaN     2025-07-05 2025-07-04   \n",
      "4  Fox Sports (ARG)      2732.0         NaN     2025-07-05 2025-07-04   \n",
      "\n",
      "                   Day  ... Spot price in Euro [1 sec.] Fixture analysis  \\\n",
      "0  2025-07-03 00:00:00  ...                         NaN              NaN   \n",
      "1  2025-07-04 00:00:00  ...                         NaN              NaN   \n",
      "2  2025-07-04 00:00:00  ...                         NaN              NaN   \n",
      "3  2025-07-04 00:00:00  ...                         NaN              NaN   \n",
      "4  2025-07-04 00:00:00  ...                         NaN              NaN   \n",
      "\n",
      "  Brandbase upload Source BT Start BT Duration Unnamed: 43 Unnamed: 44  \\\n",
      "0              NaN  IBOPE      NaN         NaN         NaN         NaN   \n",
      "1              NaN  IBOPE      NaN         NaN         NaN         NaN   \n",
      "2              NaN  IBOPE      NaN         NaN         NaN         NaN   \n",
      "3              NaN  IBOPE      NaN         NaN         NaN         NaN   \n",
      "4              NaN  IBOPE      NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 45 Unnamed: 46  \n",
      "0         NaN         NaN  \n",
      "1         NaN         NaN  \n",
      "2         NaN         NaN  \n",
      "3         NaN         NaN  \n",
      "4         NaN         NaN  \n",
      "\n",
      "[5 rows x 47 columns]\n",
      "Region                                     object\n",
      "Market                                     object\n",
      "Market ID                                 float64\n",
      "Broadcaster                                object\n",
      "TV-Channel                                 object\n",
      "Channel ID                                float64\n",
      "Pay/Free TV                                object\n",
      "Date (UTC/GMT)                     datetime64[ns]\n",
      "Date                               datetime64[ns]\n",
      "Day                                        object\n",
      "Start (UTC)                                object\n",
      "End (UTC)                                  object\n",
      "Start                                      object\n",
      "End                                        object\n",
      "Duration                                   object\n",
      "Program Title                              object\n",
      "Program Description                        object\n",
      "Combined                                   object\n",
      "Type of program                            object\n",
      "Event                                      object\n",
      "Competition                                object\n",
      "Matchday                                   object\n",
      "Phase / Fixture / Episode Desc.            object\n",
      "Home Team                                 float64\n",
      "Vs                                        float64\n",
      "Away Team                                 float64\n",
      "Gender                                    float64\n",
      "Check                                     float64\n",
      "Aud. Estimates ['000s]                    float64\n",
      "TVR% 3+                                   float64\n",
      "Aud Metered (000s) 3+                     float64\n",
      "Share% 3+                                 float64\n",
      "TVR% 14+                                  float64\n",
      "Aud Metered (000s) 14+                    float64\n",
      "Share% 14+                                float64\n",
      "CPT's [Euro]                              float64\n",
      "Spot price in Euro [30 sec.]              float64\n",
      "Spot price in Euro [1 sec.]               float64\n",
      "Fixture analysis                          float64\n",
      "Brandbase upload                          float64\n",
      "Source                                     object\n",
      "BT Start                                  float64\n",
      "BT Duration                               float64\n",
      "Unnamed: 43                               float64\n",
      "Unnamed: 44                                object\n",
      "Unnamed: 45                                object\n",
      "Unnamed: 46                                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\n",
    "    \"data/WF 3 F1-R12 - Great Britain.xlsx\",\n",
    "    sheet_name=\"Worksheet\", \n",
    "    header=5 \n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d88bb9-eda3-4685-9303-43db4bf51e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4575af76-74c0-45e9-9a31-1be4ad23d922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DataFrames ---\n",
      "âœ… Created BSR column 'Combined Programme Type' by concatenating 'Competition' and 'Type of program'.\n",
      "âœ… Renamed Overnight 'Programme Type' to 'Combined Programme Type' for matching.\n",
      "\n",
      "Total rows in Overnight file before filter: 17599\n",
      "Total rows in Overnight file after filtering on 'Grand Prix' = '15_Italian GP II': 66\n",
      "Applied date transformation: 2 rules processed.\n",
      "\n",
      "Total rows in BSR file: 2994\n",
      "Total unique ['Country', 'Channel', 'Date', 'Combined Programme Type'] combinations after filter and aggregation: 66\n",
      "âœ… Scaled BSR Audience to absolute numbers for comparison (multiplied by 1000).\n",
      "\n",
      "--- Update Summary ---\n",
      "Total rows in final BSR DataFrame: 2994\n",
      "Total BSR rows updated (where Max Overnight > BSR): 0\n",
      "No BSR audience values were updated.\n",
      "\n",
      "âœ… Updated BSR data saved to: **data/Updated_BSR_Report_AggregatedMatch_Final_CombinedKey.xlsx**\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "# Overnight Report (Source of new audience data)\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "OVERNIGHT_AUDIENCE_COL = 'Audience' # Assumed to be in absolute numbers\n",
    "\n",
    "# BSR File (File to be updated)\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "BSR_TARGET_COL = 'Aud. Estimates [\\'000s]' # This column is in thousands\n",
    "\n",
    "# â­ PROGRAMME TYPE MAPPING CONSTANT â­\n",
    "BSR_PROGRAMME_COL = 'Type of program'\n",
    "BSR_COMPETITION_COL = 'Competition'\n",
    "BSR_COMBINED_PROGRAMME_COL = 'Combined Programme Type' # New column name for merging\n",
    "\n",
    "# â­ MERGE KEYS (Updated to use the new combined column) â­\n",
    "MERGE_ON_COLS = ['Country', 'Channel', 'Date', BSR_COMBINED_PROGRAMME_COL]\n",
    "\n",
    "# Columns needed from the Overnight file \n",
    "OVERNIGHT_KEY_COLS = ['Country', 'Channel', 'Date', 'Programme Type', OVERNIGHT_AUDIENCE_COL, 'Grand Prix'] \n",
    "\n",
    "# BSR Column Mapping (Added 'Competition')\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'Market': 'Country',            \n",
    "    'Date': 'Date',                 \n",
    "    'TV-Channel': 'Channel',        \n",
    "    BSR_PROGRAMME_COL: 'Programme Type', # Keeping this intermediate map\n",
    "    BSR_COMPETITION_COL: BSR_COMPETITION_COL, # â­ Added Competition column\n",
    "    BSR_TARGET_COL: BSR_TARGET_COL  \n",
    "}\n",
    "\n",
    "# --- FILTER CONSTANTS ---\n",
    "GP_FILTER_COL = 'Grand Prix'\n",
    "GP_FILTER_VALUE = '15_Italian GP II'\n",
    "\n",
    "# --- DATE SWAP CONSTANTS ---\n",
    "DATE_SWAP_RULES = {\n",
    "    pd.to_datetime('2023-09-02'): pd.to_datetime('2023-07-05'),\n",
    "    pd.to_datetime('2023-09-03'): pd.to_datetime('2023-07-06')\n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, merge_cols, header=None):\n",
    "    \"\"\"Loads data, renames columns according to map, and standardizes key columns.\"\"\"\n",
    "    try:\n",
    "        cols_to_load = list(columns_map.keys()) \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "\n",
    "        for col in merge_cols:\n",
    "            if col in df.columns:\n",
    "                if col != 'Date':\n",
    "                    df[col] = df[col].astype(str).str.strip()\n",
    "        \n",
    "        if 'Date' in df.columns:\n",
    "             df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nâŒ Error: File not found at: **{file_path}**\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Loading DataFrames ---\")\n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    [col for col in OVERNIGHT_KEY_COLS if col != OVERNIGHT_AUDIENCE_COL],\n",
    "    header=0\n",
    ")\n",
    "if df_overnight is None:\n",
    "    print(\"Stopping execution due to Overnight file loading failure.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    # Pass all relevant keys for string cleaning, including the temporary ones\n",
    "    ['Country', 'Channel', 'Date', 'Programme Type', BSR_COMPETITION_COL],\n",
    "    header=BSR_HEADER_ROW\n",
    ")\n",
    "if df_bsr is None:\n",
    "    print(\"Stopping execution due to BSR file loading failure.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# 1. Standardize Programme Type in BSR\n",
    "# The BSR mapping renamed 'Type of program' to 'Programme Type'.\n",
    "# We combine 'Competition' and 'Programme Type' for the merge key.\n",
    "df_bsr[BSR_COMBINED_PROGRAMME_COL] = (\n",
    "    df_bsr[BSR_COMPETITION_COL].astype(str).str.strip() + \n",
    "    ' ' + \n",
    "    df_bsr['Programme Type'].astype(str).str.strip()\n",
    ")\n",
    "# We can now drop the temporary 'Competition' column\n",
    "df_bsr = df_bsr.drop(columns=[BSR_COMPETITION_COL])\n",
    "print(f\"âœ… Created BSR column '{BSR_COMBINED_PROGRAMME_COL}' by concatenating 'Competition' and 'Type of program'.\")\n",
    "\n",
    "# Rename the Overnight Programme Type column to match the new BSR column for merging\n",
    "df_overnight = df_overnight.rename(columns={'Programme Type': BSR_COMBINED_PROGRAMME_COL})\n",
    "print(f\"âœ… Renamed Overnight 'Programme Type' to '{BSR_COMBINED_PROGRAMME_COL}' for matching.\")\n",
    "\n",
    "\n",
    "# --- 2. PREPARE AND AGGREGATE OVERNIGHT DATA ---\n",
    "\n",
    "print(f\"\\nTotal rows in Overnight file before filter: {len(df_overnight)}\")\n",
    "\n",
    "# 2. APPLY THE GRAND PRIX FILTER\n",
    "if GP_FILTER_COL in df_overnight.columns:\n",
    "    df_overnight = df_overnight[df_overnight[GP_FILTER_COL] == GP_FILTER_VALUE].copy()\n",
    "    print(f\"Total rows in Overnight file after filtering on '{GP_FILTER_COL}' = '{GP_FILTER_VALUE}': {len(df_overnight)}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Warning: Overnight file is missing the required filter column: '{GP_FILTER_COL}'. Skipping filter.\")\n",
    "\n",
    "# 3. APPLY THE DATE SWAP LOGIC\n",
    "if 'Date' in df_overnight.columns:\n",
    "    for original_date, target_date in DATE_SWAP_RULES.items():\n",
    "        date_mask = df_overnight['Date'] == original_date\n",
    "        df_overnight.loc[date_mask, 'Date'] = target_date\n",
    "    print(f\"Applied date transformation: {len(DATE_SWAP_RULES)} rules processed.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: Overnight file is missing the 'Date' column. Skipping date swap.\")\n",
    "\n",
    "\n",
    "# Convert the Overnight Audience column to numeric before aggregation\n",
    "df_overnight[OVERNIGHT_AUDIENCE_COL] = pd.to_numeric(\n",
    "    df_overnight[OVERNIGHT_AUDIENCE_COL], \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal rows in BSR file: {len(df_bsr)}\")\n",
    "\n",
    "# 4. AGGREGATION STEP: Group by the final MERGE_ON_COLS (4 keys)\n",
    "df_overnight_max = df_overnight.groupby(MERGE_ON_COLS, dropna=False)[OVERNIGHT_AUDIENCE_COL].max().reset_index()\n",
    "\n",
    "# Rename the aggregated column for clarity\n",
    "df_overnight_max = df_overnight_max.rename(columns={OVERNIGHT_AUDIENCE_COL: 'Max_Overnight_Audience'})\n",
    "\n",
    "# 'Max_Overnight_Audience' remains in absolute numbers.\n",
    "\n",
    "print(f\"Total unique {MERGE_ON_COLS} combinations after filter and aggregation: {len(df_overnight_max)}\")\n",
    "\n",
    "\n",
    "# --- 3. MERGE AGGREGATED DATA AND UPDATE ---\n",
    "\n",
    "# Merge the BSR file with the simplified Overnight max audience data\n",
    "merged_df = df_bsr.merge(\n",
    "    df_overnight_max, \n",
    "    on=MERGE_ON_COLS, \n",
    "    how='left' # Use a left join to keep all BSR rows\n",
    ")\n",
    "\n",
    "# Convert BSR audience to numeric (values are in thousands)\n",
    "merged_df[BSR_TARGET_COL] = pd.to_numeric(merged_df[BSR_TARGET_COL], errors='coerce')\n",
    "\n",
    "# Scale BSR audience to absolute numbers (multiplying by 1000) FOR COMPARISON ONLY\n",
    "temp_bsr_abs = merged_df[BSR_TARGET_COL] * 1000.0\n",
    "print(\"âœ… Scaled BSR Audience to absolute numbers for comparison (multiplied by 1000).\")\n",
    "\n",
    "\n",
    "# 5. Create a boolean mask for the conditional update\n",
    "# Comparison is (Max_Overnight_Audience in absolute) > (temp_bsr_abs in absolute)\n",
    "update_mask = (merged_df['Max_Overnight_Audience'] > temp_bsr_abs) & \\\n",
    "              (merged_df[BSR_TARGET_COL].notna()) & \\\n",
    "              (merged_df['Max_Overnight_Audience'].notna())\n",
    "\n",
    "# 6. Perform the update\n",
    "rows_updated = update_mask.sum()\n",
    "\n",
    "# Update the target column in the merged DataFrame\n",
    "# Scale the absolute Overnight Audience back to thousands before writing it to the BSR column\n",
    "updated_value_in_thousands = merged_df.loc[update_mask, 'Max_Overnight_Audience'] / 1000.0\n",
    "merged_df.loc[update_mask, BSR_TARGET_COL] = updated_value_in_thousands\n",
    "\n",
    "# Copy the updated column back to the original BSR DataFrame\n",
    "df_bsr[BSR_TARGET_COL] = merged_df[BSR_TARGET_COL]\n",
    "\n",
    "\n",
    "# --- FINAL OUTPUT ---\n",
    "print(\"\\n--- Update Summary ---\")\n",
    "print(f\"Total rows in final BSR DataFrame: {len(df_bsr)}\")\n",
    "print(f\"Total BSR rows updated (where Max Overnight > BSR): {rows_updated}\")\n",
    "\n",
    "if rows_updated > 0:\n",
    "    print(\"\\n--- Sample of Updated Rows (BSR) ---\")\n",
    "    output_cols = MERGE_ON_COLS + [BSR_TARGET_COL] + ['Programme Type'] # Include the original BSR programme type for context\n",
    "    cols_to_display = [col for col in output_cols if col in df_bsr.columns]\n",
    "    print(df_bsr.loc[update_mask.index[update_mask], cols_to_display].head())\n",
    "else:\n",
    "    print(\"No BSR audience values were updated.\")\n",
    "\n",
    "# 7. Save the updated BSR to a new file (recommended)\n",
    "OUTPUT_FILE = \"data/Updated_BSR_Report_AggregatedMatch_Final_CombinedKey.xlsx\"\n",
    "df_bsr.to_excel(OUTPUT_FILE, sheet_name=BSR_SHEET, index=False)\n",
    "print(f\"\\nâœ… Updated BSR data saved to: **{OUTPUT_FILE}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0626617a-e897-4b19-9ce4-9c8a70588c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DataFrames (Simplified Logic) ---\n",
      "âœ… Renamed Overnight 'Session' and BSR 'Competition' to 'Session_Competition' for merging.\n",
      "\n",
      "Total rows in Overnight file (unfiltered): 17599\n",
      "Total rows in BSR file: 2994\n",
      "Total unique ['Country', 'Channel', 'Session_Competition'] combinations after aggregation: 650\n",
      "âœ… Scaled BSR Audience to absolute numbers for comparison (multiplied by 1000).\n",
      "\n",
      "--- Update Summary ---\n",
      "Total rows in final BSR DataFrame: 2994\n",
      "Total BSR rows updated (where Max Overnight > BSR): 0\n",
      "No BSR audience values were updated.\n",
      "\n",
      "âœ… Updated BSR data saved to: **data/Updated_BSR_Report_AggregatedMatch_Simplified_Session.xlsx**\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "# Overnight Report (Source of new audience data)\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "OVERNIGHT_AUDIENCE_COL = 'Audience' \n",
    "OVERNIGHT_SESSION_COL = 'Session' # New key from Overnight\n",
    "\n",
    "# BSR File (File to be updated)\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "BSR_TARGET_COL = 'Aud. Estimates [\\'000s]' # This column is in thousands\n",
    "BSR_COMPETITION_COL = 'Competition' # New key from BSR\n",
    "\n",
    "# â­ SIMPLIFIED MERGE KEY (Only Country, Channel, and the new Session/Competition match) â­\n",
    "MERGE_ON_COLS = ['Country', 'Channel', 'Session_Competition']\n",
    "\n",
    "# Columns needed from the Overnight file \n",
    "# Note: Keeping Date and Programme Type in the source mapping to avoid errors but they are NOT used for merging/grouping\n",
    "OVERNIGHT_KEY_COLS = ['Country', 'Channel', 'Date', 'Programme Type', OVERNIGHT_AUDIENCE_COL, OVERNIGHT_SESSION_COL]\n",
    "\n",
    "# BSR Column Mapping\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'Market': 'Country',            \n",
    "    'Date': 'Date',                 \n",
    "    'TV-Channel': 'Channel',        \n",
    "    'Type of program': 'Programme Type', \n",
    "    BSR_COMPETITION_COL: BSR_COMPETITION_COL, # Added Competition\n",
    "    BSR_TARGET_COL: BSR_TARGET_COL  \n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, merge_cols, header=None):\n",
    "    \"\"\"Loads data, renames columns according to map, and standardizes key columns.\"\"\"\n",
    "    try:\n",
    "        cols_to_load = list(columns_map.keys()) \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "\n",
    "        for col in merge_cols:\n",
    "            if col in df.columns:\n",
    "                if col != 'Date':\n",
    "                    df[col] = df[col].astype(str).str.strip()\n",
    "        \n",
    "        if 'Date' in df.columns:\n",
    "             df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nâŒ Error: File not found at: **{file_path}**\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Loading DataFrames (Simplified Logic) ---\")\n",
    "# Load Overnight data.\n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    [col for col in OVERNIGHT_KEY_COLS if col != OVERNIGHT_AUDIENCE_COL],\n",
    "    header=0\n",
    ")\n",
    "if df_overnight is None:\n",
    "    print(\"Stopping execution due to Overnight file loading failure.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Load BSR data.\n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    ['Country', 'Channel', BSR_COMPETITION_COL], # Only need these for initial cleaning\n",
    "    header=BSR_HEADER_ROW\n",
    ")\n",
    "if df_bsr is None:\n",
    "    print(\"Stopping execution due to BSR file loading failure.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# --- 1. Standardize and Prepare Merge Keys ---\n",
    "\n",
    "# â­ NEW STEP: Rename BSR 'Competition' and Overnight 'Session' to a common merge key.\n",
    "df_overnight = df_overnight.rename(columns={OVERNIGHT_SESSION_COL: 'Session_Competition'})\n",
    "df_bsr = df_bsr.rename(columns={BSR_COMPETITION_COL: 'Session_Competition'})\n",
    "print(\"âœ… Renamed Overnight 'Session' and BSR 'Competition' to 'Session_Competition' for merging.\")\n",
    "\n",
    "# --- 2. PREPARE AND AGGREGATE OVERNIGHT DATA (All filters/date swaps removed) ---\n",
    "\n",
    "print(f\"\\nTotal rows in Overnight file (unfiltered): {len(df_overnight)}\")\n",
    "\n",
    "# Convert the Overnight Audience column to numeric before aggregation\n",
    "df_overnight[OVERNIGHT_AUDIENCE_COL] = pd.to_numeric(\n",
    "    df_overnight[OVERNIGHT_AUDIENCE_COL], \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "print(f\"Total rows in BSR file: {len(df_bsr)}\")\n",
    "\n",
    "# â­ AGGREGATION STEP: Group by simplified MERGE_ON_COLS (Country, Channel, Session_Competition)\n",
    "df_overnight_max = df_overnight.groupby(MERGE_ON_COLS, dropna=False)[OVERNIGHT_AUDIENCE_COL].max().reset_index()\n",
    "\n",
    "# Rename the aggregated column for clarity\n",
    "df_overnight_max = df_overnight_max.rename(columns={OVERNIGHT_AUDIENCE_COL: 'Max_Overnight_Audience'})\n",
    "\n",
    "# 'Max_Overnight_Audience' remains in absolute numbers.\n",
    "\n",
    "print(f\"Total unique {MERGE_ON_COLS} combinations after aggregation: {len(df_overnight_max)}\")\n",
    "\n",
    "\n",
    "# --- 3. MERGE AGGREGATED DATA AND UPDATE (Scaling maintained) ---\n",
    "\n",
    "# Merge the BSR file with the simplified Overnight max audience data\n",
    "merged_df = df_bsr.merge(\n",
    "    df_overnight_max, \n",
    "    on=MERGE_ON_COLS, \n",
    "    how='left' # Use a left join to keep all BSR rows\n",
    ")\n",
    "\n",
    "# Convert BSR audience to numeric (values are in thousands)\n",
    "merged_df[BSR_TARGET_COL] = pd.to_numeric(merged_df[BSR_TARGET_COL], errors='coerce')\n",
    "\n",
    "# Scale BSR audience to absolute numbers (multiplying by 1000) FOR COMPARISON ONLY\n",
    "temp_bsr_abs = merged_df[BSR_TARGET_COL] * 1000.0\n",
    "print(\"âœ… Scaled BSR Audience to absolute numbers for comparison (multiplied by 1000).\")\n",
    "\n",
    "\n",
    "# 4. Create a boolean mask for the conditional update\n",
    "# Comparison is (Max_Overnight_Audience in absolute) > (temp_bsr_abs in absolute)\n",
    "update_mask = (merged_df['Max_Overnight_Audience'] > temp_bsr_abs) & \\\n",
    "              (merged_df[BSR_TARGET_COL].notna()) & \\\n",
    "              (merged_df['Max_Overnight_Audience'].notna())\n",
    "\n",
    "# 5. Perform the update\n",
    "rows_updated = update_mask.sum()\n",
    "\n",
    "# Update the target column in the merged DataFrame\n",
    "# Scale the absolute Overnight Audience back to thousands before writing it to the BSR column\n",
    "updated_value_in_thousands = merged_df.loc[update_mask, 'Max_Overnight_Audience'] / 1000.0\n",
    "merged_df.loc[update_mask, BSR_TARGET_COL] = updated_value_in_thousands\n",
    "\n",
    "# Copy the updated column back to the original BSR DataFrame\n",
    "df_bsr[BSR_TARGET_COL] = merged_df[BSR_TARGET_COL]\n",
    "\n",
    "\n",
    "# --- FINAL OUTPUT ---\n",
    "print(\"\\n--- Update Summary ---\")\n",
    "print(f\"Total rows in final BSR DataFrame: {len(df_bsr)}\")\n",
    "print(f\"Total BSR rows updated (where Max Overnight > BSR): {rows_updated}\")\n",
    "\n",
    "if rows_updated > 0:\n",
    "    print(\"\\n--- Sample of Updated Rows (BSR) ---\")\n",
    "    output_cols = ['Country', 'Channel', 'Session_Competition', BSR_TARGET_COL] \n",
    "    cols_to_display = [col for col in output_cols if col in df_bsr.columns]\n",
    "    print(df_bsr.loc[update_mask.index[update_mask], cols_to_display].head())\n",
    "else:\n",
    "    print(\"No BSR audience values were updated.\")\n",
    "\n",
    "# 6. Save the updated BSR to a new file (recommended)\n",
    "OUTPUT_FILE = \"data/Updated_BSR_Report_AggregatedMatch_Simplified_Session.xlsx\"\n",
    "df_bsr.to_excel(OUTPUT_FILE, sheet_name=BSR_SHEET, index=False)\n",
    "print(f\"\\nâœ… Updated BSR data saved to: **{OUTPUT_FILE}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1890fcf6-dad7-47e2-a654-c9b7699c9af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DataFrames (Final Simplified Logic with Date Swap) ---\n",
      "âœ… Renamed Overnight 'Session' and BSR 'Competition' to 'Session_Competition' for merging.\n",
      "\n",
      "Total rows in Overnight file (unfiltered): 17599\n",
      "âœ… Applied date transformation: 2 rules processed.\n",
      "Total rows in BSR file: 2994\n",
      "Total unique ['Country', 'Channel', 'Session_Competition', 'Date'] combinations after aggregation: 16641\n",
      "âœ… Scaled BSR Audience to absolute numbers for comparison (multiplied by 1000).\n",
      "\n",
      "--- DATA INSPECTION: ABSOLUTE VALUE COMPARISON (Sample) ---\n",
      "Columns:\n",
      " Max_Overnight_Audience (Absolute) vs BSR_Audience_ABS (Absolute)\n",
      "      Country              Channel Session_Competition       Date  Max_Overnight_Audience  BSR_Audience_ABS  Aud. Estimates ['000s]\n",
      "222   Belgium                VRT 1                Race 2025-07-06                 79800.0               NaN                     NaN\n",
      "814   Hungary             M4 Sport          Qualifying 2025-07-05                279900.0               NaN                     NaN\n",
      "817   Hungary             M4 Sport                Race 2025-07-06                456100.0               NaN                     NaN\n",
      "1180    Italy         Sky Sport 4K                Race 2025-07-06                 97100.0               NaN                     NaN\n",
      "1182    Italy         Sky Sport 4K                Race 2025-07-06                 97100.0               NaN                     NaN\n",
      "2450       UK      Channel 4 (GBR)          Qualifying 2025-07-05                789520.0               NaN                     NaN\n",
      "2453       UK      Channel 4 (GBR)                Race 2025-07-06               2067370.0               NaN                     NaN\n",
      "2454       UK      Channel 4 (GBR)                Race 2025-07-06               2067370.0               NaN                     NaN\n",
      "2541       UK  Sky Sports F1 (GBR)          Qualifying 2025-07-05                520340.0               NaN                     NaN\n",
      "2542       UK  Sky Sports F1 (GBR)          Qualifying 2025-07-05                520340.0               NaN                     NaN\n",
      "\n",
      "Total BSR rows where an Overnight match was found: 28\n",
      "\n",
      "--- Update Summary ---\n",
      "Total rows in final BSR DataFrame: 2994\n",
      "Total BSR rows updated (where Max Overnight > BSR): 0\n",
      "No BSR audience values were updated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHRAJG2501\\AppData\\Local\\Temp\\1\\ipykernel_45272\\841451596.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_matches['BSR_Audience_ABS'] = merged_matches[BSR_TARGET_COL] * 1000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Updated BSR data saved to: **data/Updated_BSR_Report_AggregatedMatch_Final_DateSwap.xlsx**\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "# Overnight Report (Source of new audience data)\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "OVERNIGHT_AUDIENCE_COL = 'Audience' \n",
    "OVERNIGHT_SESSION_COL = 'Session' \n",
    "\n",
    "# BSR File (File to be updated)\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "BSR_TARGET_COL = 'Aud. Estimates [\\'000s]' # This column is in thousands\n",
    "BSR_COMPETITION_COL = 'Competition' \n",
    "\n",
    "# â­ MERGE KEY (Country, Channel, and the new Session/Competition match) â­\n",
    "MERGE_ON_COLS = ['Country', 'Channel', 'Session_Competition', 'Date'] # Added Date back for matching\n",
    "\n",
    "# Columns needed from the Overnight file \n",
    "OVERNIGHT_KEY_COLS = ['Country', 'Channel', 'Date', 'Programme Type', OVERNIGHT_AUDIENCE_COL, OVERNIGHT_SESSION_COL]\n",
    "\n",
    "# BSR Column Mapping\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'Market': 'Country',            \n",
    "    'Date': 'Date',                 \n",
    "    'TV-Channel': 'Channel',        \n",
    "    'Type of program': 'Programme Type', \n",
    "    BSR_COMPETITION_COL: BSR_COMPETITION_COL, \n",
    "    BSR_TARGET_COL: BSR_TARGET_COL  \n",
    "}\n",
    "\n",
    "# â­ DATE SWAP CONSTANTS (RE-INTRODUCED) â­\n",
    "DATE_SWAP_RULES = {\n",
    "    pd.to_datetime('2023-09-02'): pd.to_datetime('2023-07-05'),\n",
    "    pd.to_datetime('2023-09-03'): pd.to_datetime('2023-07-06')\n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data (Re-used) ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, merge_cols, header=None):\n",
    "    \"\"\"Loads data, renames columns according to map, and standardizes key columns.\"\"\"\n",
    "    try:\n",
    "        cols_to_load = list(columns_map.keys()) \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "\n",
    "        for col in merge_cols:\n",
    "            if col in df.columns:\n",
    "                if col != 'Date':\n",
    "                    df[col] = df[col].astype(str).str.strip()\n",
    "        \n",
    "        if 'Date' in df.columns:\n",
    "             df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nâŒ Error: File not found at: **{file_path}**\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Loading DataFrames (Final Simplified Logic with Date Swap) ---\")\n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    [col for col in OVERNIGHT_KEY_COLS if col != OVERNIGHT_AUDIENCE_COL],\n",
    "    header=0\n",
    ")\n",
    "if df_overnight is None:\n",
    "    print(\"Stopping execution due to Overnight file loading failure.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    ['Country', 'Channel', 'Date', BSR_COMPETITION_COL], \n",
    "    header=BSR_HEADER_ROW\n",
    ")\n",
    "if df_bsr is None:\n",
    "    print(\"Stopping execution due to BSR file loading failure.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# --- 1. Standardize and Prepare Merge Keys ---\n",
    "\n",
    "# â­ NEW STEP: Rename BSR 'Competition' and Overnight 'Session' to a common merge key.\n",
    "df_overnight = df_overnight.rename(columns={OVERNIGHT_SESSION_COL: 'Session_Competition'})\n",
    "df_bsr = df_bsr.rename(columns={BSR_COMPETITION_COL: 'Session_Competition'})\n",
    "print(\"âœ… Renamed Overnight 'Session' and BSR 'Competition' to 'Session_Competition' for merging.\")\n",
    "\n",
    "# --- 2. PREPARE AND AGGREGATE OVERNIGHT DATA ---\n",
    "\n",
    "print(f\"\\nTotal rows in Overnight file (unfiltered): {len(df_overnight)}\")\n",
    "\n",
    "# â­ RE-INTRODUCED DATE SWAP LOGIC â­\n",
    "if 'Date' in df_overnight.columns:\n",
    "    for original_date, target_date in DATE_SWAP_RULES.items():\n",
    "        date_mask = df_overnight['Date'] == original_date\n",
    "        df_overnight.loc[date_mask, 'Date'] = target_date\n",
    "    print(f\"âœ… Applied date transformation: {len(DATE_SWAP_RULES)} rules processed.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: Overnight file is missing the 'Date' column. Skipping date swap.\")\n",
    "\n",
    "\n",
    "# Convert the Overnight Audience column to numeric before aggregation\n",
    "df_overnight[OVERNIGHT_AUDIENCE_COL] = pd.to_numeric(\n",
    "    df_overnight[OVERNIGHT_AUDIENCE_COL], \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "print(f\"Total rows in BSR file: {len(df_bsr)}\")\n",
    "\n",
    "# â­ AGGREGATION STEP: Group by MERGE_ON_COLS (Country, Channel, Session_Competition, Date)\n",
    "df_overnight_max = df_overnight.groupby(MERGE_ON_COLS, dropna=False)[OVERNIGHT_AUDIENCE_COL].max().reset_index()\n",
    "\n",
    "# Rename the aggregated column for clarity\n",
    "df_overnight_max = df_overnight_max.rename(columns={OVERNIGHT_AUDIENCE_COL: 'Max_Overnight_Audience'})\n",
    "\n",
    "print(f\"Total unique {MERGE_ON_COLS} combinations after aggregation: {len(df_overnight_max)}\")\n",
    "\n",
    "\n",
    "# --- 3. MERGE AGGREGATED DATA AND UPDATE ---\n",
    "\n",
    "# Merge the BSR file with the simplified Overnight max audience data\n",
    "merged_df = df_bsr.merge(\n",
    "    df_overnight_max, \n",
    "    on=MERGE_ON_COLS, \n",
    "    how='left' # Use a left join to keep all BSR rows\n",
    ")\n",
    "\n",
    "# Convert BSR audience to numeric (values are in thousands)\n",
    "merged_df[BSR_TARGET_COL] = pd.to_numeric(merged_df[BSR_TARGET_COL], errors='coerce')\n",
    "\n",
    "# Scale BSR audience to absolute numbers (multiplying by 1000) FOR COMPARISON ONLY\n",
    "temp_bsr_abs = merged_df[BSR_TARGET_COL] * 1000.0\n",
    "print(\"âœ… Scaled BSR Audience to absolute numbers for comparison (multiplied by 1000).\")\n",
    "\n",
    "# â­ NEW STEP: DATA INSPECTION LOGGING â­\n",
    "merged_matches = merged_df[merged_df['Max_Overnight_Audience'].notna()]\n",
    "if not merged_matches.empty:\n",
    "    print(\"\\n--- DATA INSPECTION: ABSOLUTE VALUE COMPARISON (Sample) ---\")\n",
    "    \n",
    "    # We must calculate the BSR Absolute value for inspection purposes\n",
    "    merged_matches['BSR_Audience_ABS'] = merged_matches[BSR_TARGET_COL] * 1000.0\n",
    "    \n",
    "    inspection_cols = ['Country', 'Channel', 'Session_Competition', 'Date', \n",
    "                       'Max_Overnight_Audience', 'BSR_Audience_ABS', BSR_TARGET_COL]\n",
    "    \n",
    "    print(\"Columns:\")\n",
    "    print(\" Max_Overnight_Audience (Absolute) vs BSR_Audience_ABS (Absolute)\")\n",
    "    print(merged_matches[inspection_cols].head(10).to_string())\n",
    "    print(f\"\\nTotal BSR rows where an Overnight match was found: {len(merged_matches)}\")\n",
    "else:\n",
    "    print(\"\\n--- DATA INSPECTION: No matches found based on the current keys. ---\")\n",
    "\n",
    "\n",
    "# 4. Create a boolean mask for the conditional update\n",
    "# Comparison is (Max_Overnight_Audience in absolute) > (temp_bsr_abs in absolute)\n",
    "update_mask = (merged_df['Max_Overnight_Audience'] > temp_bsr_abs) & \\\n",
    "              (merged_df[BSR_TARGET_COL].notna()) & \\\n",
    "              (merged_df['Max_Overnight_Audience'].notna())\n",
    "\n",
    "# 5. Perform the update\n",
    "rows_updated = update_mask.sum()\n",
    "\n",
    "# Update the target column in the merged DataFrame\n",
    "# Scale the absolute Overnight Audience back to thousands before writing it to the BSR column\n",
    "updated_value_in_thousands = merged_df.loc[update_mask, 'Max_Overnight_Audience'] / 1000.0\n",
    "merged_df.loc[update_mask, BSR_TARGET_COL] = updated_value_in_thousands\n",
    "\n",
    "# Copy the updated column back to the original BSR DataFrame\n",
    "df_bsr[BSR_TARGET_COL] = merged_df[BSR_TARGET_COL]\n",
    "\n",
    "\n",
    "# --- FINAL OUTPUT ---\n",
    "print(\"\\n--- Update Summary ---\")\n",
    "print(f\"Total rows in final BSR DataFrame: {len(df_bsr)}\")\n",
    "print(f\"Total BSR rows updated (where Max Overnight > BSR): {rows_updated}\")\n",
    "\n",
    "if rows_updated > 0:\n",
    "    print(\"\\n--- Sample of Updated Rows (BSR) ---\")\n",
    "    output_cols = MERGE_ON_COLS + [BSR_TARGET_COL] \n",
    "    cols_to_display = [col for col in output_cols if col in df_bsr.columns]\n",
    "    print(df_bsr.loc[update_mask.index[update_mask], cols_to_display].head())\n",
    "else:\n",
    "    print(\"No BSR audience values were updated.\")\n",
    "\n",
    "# 6. Save the updated BSR to a new file (recommended)\n",
    "OUTPUT_FILE = \"data/Updated_BSR_Report_AggregatedMatch_Final_DateSwap.xlsx\"\n",
    "df_bsr.to_excel(OUTPUT_FILE, sheet_name=BSR_SHEET, index=False)\n",
    "print(f\"\\nâœ… Updated BSR data saved to: **{OUTPUT_FILE}**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad605ed5-28f3-46c9-a35d-da8c3c82318d",
   "metadata": {},
   "source": [
    "Step By Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17579cbe-7139-48ff-8b3f-4b9c20183105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DataFrames for Country Match ---\n",
      "Filter applied: Overnight data now contains only '15_Dutch GP'.\n",
      "Rows before filter: 17599, Rows after filter: 185\n",
      "\n",
      "--- Match Summary ---\n",
      "Total unique countries in Overnight file (after filter): 25\n",
      "Total unique countries in BSR file: 84\n",
      "Total countries matching in both sheets: **25** ðŸŽ¯\n",
      "\n",
      "Matching Countries:\n",
      "['ARGENTINA', 'AUSTRALIA', 'AUSTRIA', 'BELGIUM', 'BRAZIL', 'BULGARIA', 'CHINA', 'DENMARK', 'FRANCE', 'GERMANY', 'GREECE', 'HUNGARY', 'IRELAND', 'ITALY', 'NETHERLANDS', 'NORWAY', 'POLAND', 'ROMANIA', 'SERBIA', 'SLOVENIA', 'SOUTH AFRICA', 'SPAIN', 'SWEDEN', 'UK', 'UNITED STATES']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "\n",
    "# â­ KEY COLUMN FOR MATCHING â­\n",
    "MATCH_COLUMN = 'Country' \n",
    "\n",
    "# â­ FILTER CONSTANTS â­\n",
    "GP_FILTER_COL = 'Grand Prix'\n",
    "GP_FILTER_VALUE = '15_Dutch GP'\n",
    "\n",
    "# Columns needed for Country match AND filtering\n",
    "OVERNIGHT_KEY_COLS = [MATCH_COLUMN, GP_FILTER_COL] \n",
    "\n",
    "# BSR Column Mapping\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'Market': MATCH_COLUMN, # Map BSR 'Market' to 'Country'\n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, header=None, use_cols=None):\n",
    "    \"\"\"Loads data and renames columns according to map.\"\"\"\n",
    "    try:\n",
    "        # Determine columns to load\n",
    "        cols_to_load = use_cols if use_cols is not None else list(columns_map.keys())\n",
    "        \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "        \n",
    "        # Standardize the match column for clean merging\n",
    "        if MATCH_COLUMN in df.columns:\n",
    "            df[MATCH_COLUMN] = df[MATCH_COLUMN].astype(str).str.strip().str.upper()\n",
    "        \n",
    "        # Standardize the filter column to prevent mismatch\n",
    "        if GP_FILTER_COL in df.columns:\n",
    "            df[GP_FILTER_COL] = df[GP_FILTER_COL].astype(str).str.strip()\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Loading DataFrames for Country Match ---\")\n",
    "\n",
    "# 1. Load Overnight data (Now includes 'Grand Prix' for filtering)\n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    header=0,\n",
    "    use_cols=list({col: col for col in OVERNIGHT_KEY_COLS}.keys())\n",
    ")\n",
    "if df_overnight is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# â­ APPLY THE FILTER â­\n",
    "rows_before_filter = len(df_overnight)\n",
    "if GP_FILTER_COL in df_overnight.columns:\n",
    "    df_overnight = df_overnight[df_overnight[GP_FILTER_COL] == GP_FILTER_VALUE].copy()\n",
    "    print(f\"Filter applied: Overnight data now contains only '{GP_FILTER_VALUE}'.\")\n",
    "    print(f\"Rows before filter: {rows_before_filter}, Rows after filter: {len(df_overnight)}\")\n",
    "else:\n",
    "    print(f\"Warning: Overnight file is missing the column '{GP_FILTER_COL}'. Skipping filter.\")\n",
    "\n",
    "\n",
    "# 2. Load BSR data (Only need the Country column)\n",
    "# Loading all columns initially to avoid usecols errors, then renaming.\n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    header=BSR_HEADER_ROW,\n",
    "    use_cols=None # Load all columns\n",
    ")\n",
    "if df_bsr is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# --- Find Unique Matching Countries ---\n",
    "\n",
    "# 3. Get unique countries from each DataFrame\n",
    "unique_countries_overnight = set(df_overnight[MATCH_COLUMN].unique())\n",
    "unique_countries_bsr = set(df_bsr[MATCH_COLUMN].unique())\n",
    "\n",
    "# 4. Find the intersection (the countries present in both)\n",
    "matching_countries = unique_countries_overnight.intersection(unique_countries_bsr)\n",
    "\n",
    "print(\"\\n--- Match Summary ---\")\n",
    "print(f\"Total unique countries in Overnight file (after filter): {len(unique_countries_overnight)}\")\n",
    "print(f\"Total unique countries in BSR file: {len(unique_countries_bsr)}\")\n",
    "print(f\"Total countries matching in both sheets: **{len(matching_countries)}** ðŸŽ¯\")\n",
    "\n",
    "# 5. Print the list of matching countries\n",
    "if len(matching_countries) > 0:\n",
    "    print(\"\\nMatching Countries:\")\n",
    "    # Convert set back to list, sort, and print nicely\n",
    "    print(sorted(list(matching_countries)))\n",
    "else:\n",
    "    print(\"No matching countries found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf55894-2431-4adf-ac82-b71f66c4c445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DataFrames for Date Match ---\n",
      "âœ… Applied date transformation: 3 rules processed on Overnight data.\n",
      "Filter applied: Overnight data now contains only '15_Dutch GP'.\n",
      "Rows before filter: 17599, Rows after filter: 185\n",
      "âœ… Converted both 'Date' columns to 'YYYY-MM-DD' string key for reliable comparison.\n",
      "\n",
      "--- Date Match Summary (After Format Fix) ---\n",
      "Total unique dates in Overnight file (post-filter/swap): 8\n",
      "Total unique dates in BSR file: 7\n",
      "Total dates matching in both sheets: **2** ðŸŽ¯\n",
      "\n",
      "Matching Dates (YYYY-MM-DD):\n",
      "['2025-07-05', '2025-07-06']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "\n",
    "# â­ KEY COLUMN FOR MATCHING â­\n",
    "MATCH_COLUMN = 'Date' \n",
    "COUNTRY_COLUMN = 'Country'\n",
    "\n",
    "# â­ FILTER CONSTANTS â­\n",
    "GP_FILTER_COL = 'Grand Prix'\n",
    "GP_FILTER_VALUE = '15_Dutch GP'\n",
    "\n",
    "# â­ UPDATED DATE SWAP CONSTANTS â­\n",
    "DATE_SWAP_RULES = {\n",
    "    # {Original Date: Target Date}\n",
    "    pd.to_datetime('2025-08-30'): pd.to_datetime('2025-07-05'),\n",
    "    pd.to_datetime('2025-08-31'): pd.to_datetime('2025-07-06'),\n",
    "    pd.to_datetime('2025-07-06'): pd.to_datetime('2025-07-06') # Retains the date if it's already 06-07-2025\n",
    "}\n",
    "\n",
    "# Columns needed for Date match, Country standardization, and filtering\n",
    "OVERNIGHT_KEY_COLS = [MATCH_COLUMN, GP_FILTER_COL, COUNTRY_COLUMN] \n",
    "\n",
    "# BSR Column Mapping\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'Market': COUNTRY_COLUMN,\n",
    "    'Date': MATCH_COLUMN,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data (Reusing previous logic) ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, header=None, use_cols=None):\n",
    "    \"\"\"Loads data, renames columns according to map, and standardizes key columns.\"\"\"\n",
    "    try:\n",
    "        cols_to_load = use_cols if use_cols is not None else list(columns_map.keys())\n",
    "        \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "        \n",
    "        if COUNTRY_COLUMN in df.columns:\n",
    "            df[COUNTRY_COLUMN] = df[COUNTRY_COLUMN].astype(str).str.strip().str.upper()\n",
    "        if GP_FILTER_COL in df.columns:\n",
    "            df[GP_FILTER_COL] = df[GP_FILTER_COL].astype(str).str.strip()\n",
    "        \n",
    "        # Convert Date column to datetime (essential for the swap to work)\n",
    "        if MATCH_COLUMN in df.columns:\n",
    "             # Errors='coerce' will turn problematic entries into NaT\n",
    "             df[MATCH_COLUMN] = pd.to_datetime(df[MATCH_COLUMN], errors='coerce')\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Loading DataFrames for Date Match ---\")\n",
    "\n",
    "# 1. Load Overnight data (Includes Date, Country, and Grand Prix)\n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    header=0,\n",
    "    use_cols=list({col: col for col in OVERNIGHT_KEY_COLS}.keys())\n",
    ")\n",
    "if df_overnight is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# â­ STEP A: APPLY DATE SWAP LOGIC (IMMEDIATELY AFTER LOADING) â­\n",
    "if MATCH_COLUMN in df_overnight.columns:\n",
    "    for original_date, target_date in DATE_SWAP_RULES.items():\n",
    "        if isinstance(original_date, pd.Timestamp):\n",
    "             date_mask = df_overnight[MATCH_COLUMN] == original_date\n",
    "             df_overnight.loc[date_mask, MATCH_COLUMN] = target_date\n",
    "    print(f\"âœ… Applied date transformation: {len(DATE_SWAP_RULES)} rules processed on Overnight data.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: Overnight file is missing the 'Date' column. Skipping date swap.\")\n",
    "\n",
    "\n",
    "# â­ STEP B: APPLY THE GRAND PRIX FILTER (AFTER DATE SWAP) â­\n",
    "rows_before_filter = len(df_overnight)\n",
    "if GP_FILTER_COL in df_overnight.columns:\n",
    "    df_overnight = df_overnight[df_overnight[GP_FILTER_COL] == GP_FILTER_VALUE].copy()\n",
    "    print(f\"Filter applied: Overnight data now contains only '{GP_FILTER_VALUE}'.\")\n",
    "    print(f\"Rows before filter: {rows_before_filter}, Rows after filter: {len(df_overnight)}\")\n",
    "else:\n",
    "    print(f\"Warning: Overnight file is missing the column '{GP_FILTER_COL}'. Skipping filter.\")\n",
    "\n",
    "\n",
    "# 2. Load BSR data (Only need Date and Country)\n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    header=BSR_HEADER_ROW,\n",
    "    use_cols=None \n",
    ")\n",
    "if df_bsr is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# --- Find Unique Matching Dates (With Format Fix) ---\n",
    "\n",
    "# â­ FIX: Convert dates to YYYY-MM-DD string format for guaranteed matching â­\n",
    "df_overnight['Date_Key'] = df_overnight[MATCH_COLUMN].dt.strftime('%Y-%m-%d').astype(str)\n",
    "df_bsr['Date_Key'] = df_bsr[MATCH_COLUMN].dt.strftime('%Y-%m-%d').astype(str)\n",
    "print(\"âœ… Converted both 'Date' columns to 'YYYY-MM-DD' string key for reliable comparison.\")\n",
    "\n",
    "\n",
    "# 3. Get unique dates from each DataFrame (using the new string key)\n",
    "unique_dates_overnight = set(df_overnight['Date_Key'].dropna().unique())\n",
    "unique_dates_bsr = set(df_bsr['Date_Key'].dropna().unique())\n",
    "# Clean up 'NaT' strings which result from failed date parsing\n",
    "if 'NaT' in unique_dates_overnight: unique_dates_overnight.remove('NaT')\n",
    "if 'NaT' in unique_dates_bsr: unique_dates_bsr.remove('NaT')\n",
    "\n",
    "\n",
    "# 4. Find the intersection (the dates present in both)\n",
    "matching_dates = unique_dates_overnight.intersection(unique_dates_bsr)\n",
    "\n",
    "print(\"\\n--- Date Match Summary (After Format Fix) ---\")\n",
    "print(f\"Total unique dates in Overnight file (post-filter/swap): {len(unique_dates_overnight)}\")\n",
    "print(f\"Total unique dates in BSR file: {len(unique_dates_bsr)}\")\n",
    "print(f\"Total dates matching in both sheets: **{len(matching_dates)}** ðŸŽ¯\")\n",
    "\n",
    "# 5. Print the list of matching dates\n",
    "if len(matching_dates) > 0:\n",
    "    print(\"\\nMatching Dates (YYYY-MM-DD):\")\n",
    "    print(sorted(list(matching_dates)))\n",
    "else:\n",
    "    print(\"No matching dates found even after format correction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9400a495-1801-4a7a-a897-05e9eebfe016",
   "metadata": {},
   "source": [
    "Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02454979-b8ad-4395-ad88-07321d309769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DataFrames for Channel Match ---\n",
      "âœ… Applied date transformation: 3 rules processed on Overnight data.\n",
      "Filter applied: Overnight data now contains only '15_Dutch GP'.\n",
      "Rows before filter: 17599, Rows after filter: 185\n",
      "\n",
      "--- Channel Match Summary ---\n",
      "Total unique channels in Overnight file (post-filter/swap): 57\n",
      "Total unique channels in BSR file: 240\n",
      "Total channels matching in both sheets: **10** ðŸŽ¯\n",
      "\n",
      "Matching Channels (Standardized):\n",
      "['CANAL+', 'CHANNEL 4 (GBR)', 'ESPN', 'ESPNU', 'M4 SPORT', 'SKY SPORT 4K', 'SKY SPORTS F1 (GBR)', 'SKY SPORTS MAIN EVENT (GBR)', 'TIPIK', 'VRT 1']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "\n",
    "# â­ KEY COLUMN FOR MATCHING (CHANGED) â­\n",
    "MATCH_COLUMN = 'Channel' \n",
    "COUNTRY_COLUMN = 'Country' \n",
    "DATE_COLUMN = 'Date'\n",
    "\n",
    "# â­ FILTER CONSTANTS (Retained) â­\n",
    "GP_FILTER_COL = 'Grand Prix'\n",
    "GP_FILTER_VALUE = '15_Dutch GP'\n",
    "\n",
    "# â­ DATE SWAP CONSTANTS (Retained) â­\n",
    "DATE_SWAP_RULES = {\n",
    "    pd.to_datetime('2025-08-30'): pd.to_datetime('2025-07-05'),\n",
    "    pd.to_datetime('2025-08-31'): pd.to_datetime('2025-07-06'),\n",
    "    pd.to_datetime('2025-07-06'): pd.to_datetime('2025-07-06') \n",
    "}\n",
    "\n",
    "# Columns needed for Channel match, Country, Date, and filtering\n",
    "OVERNIGHT_KEY_COLS = [MATCH_COLUMN, DATE_COLUMN, GP_FILTER_COL, COUNTRY_COLUMN] \n",
    "\n",
    "# BSR Column Mapping\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'TV-Channel': MATCH_COLUMN, # Map BSR 'TV-Channel' to 'Channel'\n",
    "    'Market': COUNTRY_COLUMN,\n",
    "    'Date': DATE_COLUMN,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, header=None, use_cols=None):\n",
    "    \"\"\"Loads data, renames columns according to map, and standardizes key columns.\"\"\"\n",
    "    try:\n",
    "        cols_to_load = use_cols if use_cols is not None else list(columns_map.keys())\n",
    "        \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "        \n",
    "        # Standardize Country and Filter columns\n",
    "        if COUNTRY_COLUMN in df.columns:\n",
    "            df[COUNTRY_COLUMN] = df[COUNTRY_COLUMN].astype(str).str.strip().str.upper()\n",
    "        if GP_FILTER_COL in df.columns:\n",
    "            df[GP_FILTER_COL] = df[GP_FILTER_COL].astype(str).str.strip()\n",
    "        \n",
    "        # Convert Date column to datetime\n",
    "        if DATE_COLUMN in df.columns:\n",
    "             df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN], errors='coerce')\n",
    "        \n",
    "        # Standardize Channel column (Crucial for this step)\n",
    "        if MATCH_COLUMN in df.columns:\n",
    "             df[MATCH_COLUMN] = df[MATCH_COLUMN].astype(str).str.strip().str.upper()\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Loading DataFrames for Channel Match ---\")\n",
    "\n",
    "# 1. Load Overnight data \n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    header=0,\n",
    "    use_cols=list({col: col for col in OVERNIGHT_KEY_COLS}.keys())\n",
    ")\n",
    "if df_overnight is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# â­ STEP A: APPLY DATE SWAP LOGIC â­\n",
    "if DATE_COLUMN in df_overnight.columns:\n",
    "    for original_date, target_date in DATE_SWAP_RULES.items():\n",
    "        if isinstance(original_date, pd.Timestamp):\n",
    "             date_mask = df_overnight[DATE_COLUMN] == original_date\n",
    "             df_overnight.loc[date_mask, DATE_COLUMN] = target_date\n",
    "    print(f\"âœ… Applied date transformation: {len(DATE_SWAP_RULES)} rules processed on Overnight data.\")\n",
    "\n",
    "\n",
    "# â­ STEP B: APPLY THE GRAND PRIX FILTER â­\n",
    "rows_before_filter = len(df_overnight)\n",
    "if GP_FILTER_COL in df_overnight.columns:\n",
    "    df_overnight = df_overnight[df_overnight[GP_FILTER_COL] == GP_FILTER_VALUE].copy()\n",
    "    print(f\"Filter applied: Overnight data now contains only '{GP_FILTER_VALUE}'.\")\n",
    "    print(f\"Rows before filter: {rows_before_filter}, Rows after filter: {len(df_overnight)}\")\n",
    "\n",
    "\n",
    "# 2. Load BSR data \n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    header=BSR_HEADER_ROW,\n",
    "    use_cols=None \n",
    ")\n",
    "if df_bsr is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# --- Find Unique Matching Channels ---\n",
    "\n",
    "# 3. Get unique channels from each DataFrame (Standardized to upper case string)\n",
    "unique_channels_overnight = set(df_overnight[MATCH_COLUMN].dropna().unique())\n",
    "unique_channels_bsr = set(df_bsr[MATCH_COLUMN].dropna().unique())\n",
    "\n",
    "# 4. Find the intersection (the channels present in both)\n",
    "matching_channels = unique_channels_overnight.intersection(unique_channels_bsr)\n",
    "\n",
    "print(\"\\n--- Channel Match Summary ---\")\n",
    "print(f\"Total unique channels in Overnight file (post-filter/swap): {len(unique_channels_overnight)}\")\n",
    "print(f\"Total unique channels in BSR file: {len(unique_channels_bsr)}\")\n",
    "print(f\"Total channels matching in both sheets: **{len(matching_channels)}** ðŸŽ¯\")\n",
    "\n",
    "# 5. Print the list of matching channels\n",
    "if len(matching_channels) > 0:\n",
    "    print(\"\\nMatching Channels (Standardized):\")\n",
    "    print(sorted(list(matching_channels)))\n",
    "else:\n",
    "    print(\"No matching channels found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69a0fa-f12f-4b87-9225-21c54633f165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d42b6691-462a-4927-9e2e-8e3e0b54ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DataFrames for Combined Key Match (Country, Date, Channel) ---\n",
      "âœ… Applied date transformation: 3 rules processed on Overnight data.\n",
      "Filter applied: Overnight data now contains only '15_Dutch GP'.\n",
      "Rows before filter: 17599, Rows after filter: 185\n",
      "\n",
      "--- Combined Match Summary (Inner Join) ---\n",
      "Total Unique Keys matched (Country, Date, Channel): **13** ðŸŽ¯\n",
      "These combinations contain the following unique counts:\n",
      "* Countries Matched: 5\n",
      "* Dates Matched: 2\n",
      "* Channels Matched: 8\n",
      "\n",
      "Sample of Channels found in matching combinations:\n",
      "['CHANNEL 4 (GBR)', 'ESPNU', 'M4 SPORT', 'SKY SPORT 4K', 'SKY SPORTS F1 (GBR)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "\n",
    "# â­ FULL MERGE KEY FOR THIS STEP â­\n",
    "MERGE_ON_COLS = ['Country', 'Channel', 'Date']\n",
    "COUNTRY_COLUMN = 'Country' \n",
    "DATE_COLUMN = 'Date'\n",
    "CHANNEL_COLUMN = 'Channel'\n",
    "\n",
    "# â­ FILTER CONSTANTS (Retained) â­\n",
    "GP_FILTER_COL = 'Grand Prix'\n",
    "GP_FILTER_VALUE = '15_Dutch GP'\n",
    "\n",
    "# â­ DATE SWAP CONSTANTS (Retained) â­\n",
    "DATE_SWAP_RULES = {\n",
    "    pd.to_datetime('2025-08-30'): pd.to_datetime('2025-07-05'),\n",
    "    pd.to_datetime('2025-08-31'): pd.to_datetime('2025-07-06'),\n",
    "    pd.to_datetime('2025-07-06'): pd.to_datetime('2025-07-06') \n",
    "}\n",
    "\n",
    "# Columns needed from Overnight\n",
    "OVERNIGHT_KEY_COLS = MERGE_ON_COLS + [GP_FILTER_COL] \n",
    "\n",
    "# BSR Column Mapping\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'TV-Channel': CHANNEL_COLUMN, \n",
    "    'Market': COUNTRY_COLUMN,\n",
    "    'Date': DATE_COLUMN,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, header=None, use_cols=None):\n",
    "    \"\"\"Loads data, renames columns according to map, and standardizes key columns.\"\"\"\n",
    "    try:\n",
    "        cols_to_load = use_cols if use_cols is not None else list(columns_map.keys())\n",
    "        \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "        \n",
    "        # Standardize String Columns (Country and Channel)\n",
    "        for col in [COUNTRY_COLUMN, CHANNEL_COLUMN]:\n",
    "            if col in df.columns:\n",
    "                 df[col] = df[col].astype(str).str.strip().str.upper()\n",
    "\n",
    "        # Standardize Filter Column\n",
    "        if GP_FILTER_COL in df.columns:\n",
    "            df[GP_FILTER_COL] = df[GP_FILTER_COL].astype(str).str.strip()\n",
    "        \n",
    "        # Convert Date column to datetime\n",
    "        if DATE_COLUMN in df.columns:\n",
    "             df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN], errors='coerce')\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Loading DataFrames for Combined Key Match (Country, Date, Channel) ---\")\n",
    "\n",
    "# 1. Load Overnight data \n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    header=0,\n",
    "    use_cols=list({col: col for col in OVERNIGHT_KEY_COLS}.keys())\n",
    ")\n",
    "if df_overnight is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# â­ STEP A: APPLY DATE SWAP LOGIC â­\n",
    "if DATE_COLUMN in df_overnight.columns:\n",
    "    for original_date, target_date in DATE_SWAP_RULES.items():\n",
    "        if isinstance(original_date, pd.Timestamp):\n",
    "             date_mask = df_overnight[DATE_COLUMN] == original_date\n",
    "             df_overnight.loc[date_mask, DATE_COLUMN] = target_date\n",
    "    print(f\"âœ… Applied date transformation: {len(DATE_SWAP_RULES)} rules processed on Overnight data.\")\n",
    "\n",
    "\n",
    "# â­ STEP B: APPLY THE GRAND PRIX FILTER â­\n",
    "rows_before_filter = len(df_overnight)\n",
    "if GP_FILTER_COL in df_overnight.columns:\n",
    "    df_overnight = df_overnight[df_overnight[GP_FILTER_COL] == GP_FILTER_VALUE].copy()\n",
    "    print(f\"Filter applied: Overnight data now contains only '{GP_FILTER_VALUE}'.\")\n",
    "    print(f\"Rows before filter: {rows_before_filter}, Rows after filter: {len(df_overnight)}\")\n",
    "\n",
    "\n",
    "# 2. Load BSR data \n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    header=BSR_HEADER_ROW,\n",
    "    use_cols=None \n",
    ")\n",
    "if df_bsr is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "\n",
    "# --- Find Unique Matching Combinations (Inner Join) ---\n",
    "\n",
    "# 3. Drop all non-key columns from both DFs and then drop duplicates\n",
    "overnight_unique_keys = df_overnight[MERGE_ON_COLS].drop_duplicates().dropna()\n",
    "bsr_unique_keys = df_bsr[MERGE_ON_COLS].drop_duplicates().dropna()\n",
    "\n",
    "# 4. Perform an INNER MERGE to find combinations present in BOTH\n",
    "matching_combinations_df = overnight_unique_keys.merge(\n",
    "    bsr_unique_keys,\n",
    "    on=MERGE_ON_COLS,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# 5. Extract results\n",
    "matching_countries = matching_combinations_df['Country'].nunique()\n",
    "matching_dates = matching_combinations_df['Date'].nunique()\n",
    "matching_channels = matching_combinations_df['Channel'].nunique()\n",
    "total_unique_combinations = len(matching_combinations_df)\n",
    "\n",
    "\n",
    "print(\"\\n--- Combined Match Summary (Inner Join) ---\")\n",
    "print(f\"Total Unique Keys matched (Country, Date, Channel): **{total_unique_combinations}** ðŸŽ¯\")\n",
    "print(\"These combinations contain the following unique counts:\")\n",
    "print(f\"* Countries Matched: {matching_countries}\")\n",
    "print(f\"* Dates Matched: {matching_dates}\")\n",
    "print(f\"* Channels Matched: {matching_channels}\")\n",
    "\n",
    "# 6. Print a sample of the actual matching channels\n",
    "if matching_channels > 0:\n",
    "    print(\"\\nSample of Channels found in matching combinations:\")\n",
    "    print(sorted(matching_combinations_df['Channel'].unique())[:5])\n",
    "else:\n",
    "    print(\"No unique Country-Date-Channel combinations were found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a603bfb-78a9-4552-93bb-3bcd83759eed",
   "metadata": {},
   "source": [
    "session vs Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd8b523-5bf8-46c1-b14e-be7348832ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DataFrames for FULL Key Match ---\n",
      "âœ… Applied date transformation: 3 rules processed on Overnight data.\n",
      "Filter applied: Overnight data now contains only '15_Dutch GP'.\n",
      "Rows before filter: 17599, Rows after filter: 185\n",
      "âœ… Forced Overnight 'Session' to 'QUALIFYING' on 2025-07-05 and 'RACE' on 2025-07-06.\n",
      "\n",
      "Total unique key combinations in Overnight (pre-merge): 184\n",
      "Total unique key combinations in BSR (pre-merge): 1379\n",
      "\n",
      "--- FINAL Combined Match Summary (Inner Join) ---\n",
      "Total Unique Slots Matched (Country, Channel, Date, Session/Competition): **12** ðŸŽ¯\n",
      "These matched slots contain the following unique counts:\n",
      "* Countries Matched: 4\n",
      "* Dates Matched: 2\n",
      "* Channels Matched: 7\n",
      "* Session/Competition Matched: 2\n",
      "\n",
      "Sample of matched key combinations (Country, Date, Session/Competition):\n",
      "Country       Date Session_Competition                     Channel\n",
      "HUNGARY 2025-07-05          QUALIFYING                    M4 SPORT\n",
      "HUNGARY 2025-07-06                RACE                    M4 SPORT\n",
      "  ITALY 2025-07-05          QUALIFYING                SKY SPORT 4K\n",
      "  ITALY 2025-07-06                RACE                SKY SPORT 4K\n",
      "     UK 2025-07-05          QUALIFYING             CHANNEL 4 (GBR)\n",
      "     UK 2025-07-06                RACE             CHANNEL 4 (GBR)\n",
      "     UK 2025-07-05          QUALIFYING         SKY SPORTS F1 (GBR)\n",
      "     UK 2025-07-06                RACE         SKY SPORTS F1 (GBR)\n",
      "     UK 2025-07-05          QUALIFYING SKY SPORTS MAIN EVENT (GBR)\n",
      "BELGIUM 2025-07-06                RACE                       TIPIK\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "\n",
    "# â­ FINAL MERGE KEY COLUMNS â­\n",
    "COUNTRY_COLUMN = 'Country' \n",
    "DATE_COLUMN = 'Date'\n",
    "CHANNEL_COLUMN = 'Channel'\n",
    "SESSION_COMPETITION_COLUMN = 'Session_Competition'\n",
    "\n",
    "FINAL_MERGE_ON_COLS = [COUNTRY_COLUMN, CHANNEL_COLUMN, DATE_COLUMN, SESSION_COMPETITION_COLUMN]\n",
    "\n",
    "# â­ FILTER & SWAP CONSTANTS (Retained) â­\n",
    "GP_FILTER_COL = 'Grand Prix'\n",
    "GP_FILTER_VALUE = '15_Dutch GP'\n",
    "\n",
    "DATE_SWAP_RULES = {\n",
    "    pd.to_datetime('2025-08-30'): pd.to_datetime('2025-07-05'),\n",
    "    pd.to_datetime('2025-08-31'): pd.to_datetime('2025-07-06'),\n",
    "    pd.to_datetime('2025-07-06'): pd.to_datetime('2025-07-06') \n",
    "}\n",
    "\n",
    "# Overnight Columns needed\n",
    "OVERNIGHT_KEY_COLS = [COUNTRY_COLUMN, CHANNEL_COLUMN, DATE_COLUMN, 'Session', GP_FILTER_COL] \n",
    "\n",
    "# BSR Column Mapping\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'TV-Channel': CHANNEL_COLUMN, \n",
    "    'Market': COUNTRY_COLUMN,\n",
    "    'Date': DATE_COLUMN,\n",
    "    'Competition': SESSION_COMPETITION_COLUMN # Map BSR 'Competition' to the common key\n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, header=None, use_cols=None):\n",
    "    \"\"\"Loads data, renames columns according to map, and standardizes key columns.\"\"\"\n",
    "    try:\n",
    "        cols_to_load = use_cols if use_cols is not None else list(columns_map.keys())\n",
    "        \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "        \n",
    "        # Standardize String Columns (Country and Channel)\n",
    "        for col in [COUNTRY_COLUMN, CHANNEL_COLUMN, SESSION_COMPETITION_COLUMN]:\n",
    "            if col in df.columns:\n",
    "                 df[col] = df[col].astype(str).str.strip().str.upper()\n",
    "\n",
    "        # Standardize Filter Column\n",
    "        if GP_FILTER_COL in df.columns:\n",
    "            df[GP_FILTER_COL] = df[GP_FILTER_COL].astype(str).str.strip()\n",
    "        \n",
    "        # Convert Date column to datetime\n",
    "        if DATE_COLUMN in df.columns:\n",
    "             df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN], errors='coerce')\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Loading DataFrames for FULL Key Match ---\")\n",
    "\n",
    "# 1. Load Overnight data \n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    header=0,\n",
    "    use_cols=list({col: col for col in OVERNIGHT_KEY_COLS}.keys())\n",
    ")\n",
    "if df_overnight is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# â­ STEP A: APPLY DATE SWAP LOGIC â­\n",
    "if DATE_COLUMN in df_overnight.columns:\n",
    "    for original_date, target_date in DATE_SWAP_RULES.items():\n",
    "        if isinstance(original_date, pd.Timestamp):\n",
    "             date_mask = df_overnight[DATE_COLUMN] == original_date\n",
    "             df_overnight.loc[date_mask, DATE_COLUMN] = target_date\n",
    "    print(f\"âœ… Applied date transformation: {len(DATE_SWAP_RULES)} rules processed on Overnight data.\")\n",
    "\n",
    "\n",
    "# â­ STEP B: APPLY THE GRAND PRIX FILTER â­\n",
    "rows_before_filter = len(df_overnight)\n",
    "if GP_FILTER_COL in df_overnight.columns:\n",
    "    df_overnight = df_overnight[df_overnight[GP_FILTER_COL] == GP_FILTER_VALUE].copy()\n",
    "    print(f\"Filter applied: Overnight data now contains only '{GP_FILTER_VALUE}'.\")\n",
    "    print(f\"Rows before filter: {rows_before_filter}, Rows after filter: {len(df_overnight)}\")\n",
    "\n",
    "\n",
    "# â­ STEP C: FORCE SESSION ALIGNMENT (NEW LOGIC) â­\n",
    "TARGET_DATE_QUALIFYING = pd.to_datetime('2025-07-05')\n",
    "TARGET_DATE_RACE = pd.to_datetime('2025-07-06')\n",
    "SESSION_COL_NAME = 'Session' # Original column name in df_overnight\n",
    "\n",
    "if SESSION_COL_NAME in df_overnight.columns and DATE_COLUMN in df_overnight.columns:\n",
    "    # 1. Set 'Session' to 'QUALIFYING' for 2025-07-05\n",
    "    qualifying_mask = df_overnight[DATE_COLUMN] == TARGET_DATE_QUALIFYING\n",
    "    df_overnight.loc[qualifying_mask, SESSION_COL_NAME] = 'QUALIFYING'\n",
    "\n",
    "    # 2. Set 'Session' to 'RACE' for 2025-07-06\n",
    "    race_mask = df_overnight[DATE_COLUMN] == TARGET_DATE_RACE\n",
    "    df_overnight.loc[race_mask, SESSION_COL_NAME] = 'RACE'\n",
    "    \n",
    "    print(f\"âœ… Forced Overnight '{SESSION_COL_NAME}' to 'QUALIFYING' on {TARGET_DATE_QUALIFYING.date()} and 'RACE' on {TARGET_DATE_RACE.date()}.\")\n",
    "\n",
    "\n",
    "# â­ STEP D: Map Overnight 'Session' to 'Session_Competition' â­\n",
    "# Note: Since the new values are ALL CAPS, they will align with the standardization done on BSR's 'Competition' column.\n",
    "df_overnight = df_overnight.rename(columns={'Session': SESSION_COMPETITION_COLUMN})\n",
    "\n",
    "\n",
    "# 2. Load BSR data \n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    header=BSR_HEADER_ROW,\n",
    "    use_cols=None \n",
    ")\n",
    "if df_bsr is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "\n",
    "# --- Find Unique Matching Combinations (Inner Join) ---\n",
    "\n",
    "# 3. Drop all non-key columns from both DFs and then drop duplicates\n",
    "overnight_unique_keys = df_overnight[FINAL_MERGE_ON_COLS].drop_duplicates().dropna()\n",
    "bsr_unique_keys = df_bsr[FINAL_MERGE_ON_COLS].drop_duplicates().dropna()\n",
    "\n",
    "print(f\"\\nTotal unique key combinations in Overnight (pre-merge): {len(overnight_unique_keys)}\")\n",
    "print(f\"Total unique key combinations in BSR (pre-merge): {len(bsr_unique_keys)}\")\n",
    "\n",
    "\n",
    "# 4. Perform an INNER MERGE to find combinations present in BOTH\n",
    "matching_combinations_df = overnight_unique_keys.merge(\n",
    "    bsr_unique_keys,\n",
    "    on=FINAL_MERGE_ON_COLS,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# 5. Extract results\n",
    "matching_countries = matching_combinations_df['Country'].nunique()\n",
    "matching_dates = matching_combinations_df['Date'].nunique()\n",
    "matching_channels = matching_combinations_df['Channel'].nunique()\n",
    "matching_session_competition = matching_combinations_df[SESSION_COMPETITION_COLUMN].nunique()\n",
    "total_unique_combinations = len(matching_combinations_df)\n",
    "\n",
    "\n",
    "print(\"\\n--- FINAL Combined Match Summary (Inner Join) ---\")\n",
    "print(f\"Total Unique Slots Matched (Country, Channel, Date, Session/Competition): **{total_unique_combinations}** ðŸŽ¯\")\n",
    "print(\"These matched slots contain the following unique counts:\")\n",
    "print(f\"* Countries Matched: {matching_countries}\")\n",
    "print(f\"* Dates Matched: {matching_dates}\")\n",
    "print(f\"* Channels Matched: {matching_channels}\")\n",
    "print(f\"* Session/Competition Matched: {matching_session_competition}\")\n",
    "\n",
    "# 6. Print a sample of the actual matching sessions/competitions\n",
    "if total_unique_combinations > 0:\n",
    "    print(\"\\nSample of matched key combinations (Country, Date, Session/Competition):\")\n",
    "    output_cols = [COUNTRY_COLUMN, DATE_COLUMN, SESSION_COMPETITION_COLUMN, CHANNEL_COLUMN]\n",
    "    print(matching_combinations_df[output_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"No unique Country-Channel-Date-Session/Competition combinations were found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc6643-d696-4060-baf0-926874bc9861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb4b70af-5f4e-4ab5-b9ef-8a0a0ac0fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Final Audience Update Process ---\n",
      "âœ… Applied date transformation: 3 rules processed.\n",
      "âœ… Filtered Overnight data to only '15_Dutch GP'. Rows: 185\n",
      "âœ… Forced Overnight 'Session' alignment for QUALIFYING and RACE dates.\n",
      "Total rows in BSR file: 2994\n",
      "\n",
      "--- Update Summary ---\n",
      "Total rows in final BSR DataFrame: 2994\n",
      "Total BSR rows updated (where Max Overnight > BSR): 14\n",
      "\n",
      "âœ… Updated BSR data saved to: **data/Updated_BSR_Report_Final_v2.xlsx**\n",
      "\n",
      "--- Sample of Updated Rows (BSR) ---\n",
      "Country      Channel       Date Session_Competition  Aud Metered (000s) 3+\n",
      "BELGIUM        TIPIK 2025-07-05          QUALIFYING                  101.0\n",
      "BELGIUM        TIPIK 2025-07-06                RACE                  181.9\n",
      "HUNGARY     M4 SPORT 2025-07-06                RACE                  486.6\n",
      "  ITALY SKY SPORT 4K 2025-07-05          QUALIFYING                   67.1\n",
      "  ITALY SKY SPORT 4K 2025-07-06                RACE                  138.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- FILE DEFINITIONS ---\n",
    "OVERNIGHT_FILE = \"data/F12025 Quick Report Audiences (24 market).xlsx\" \n",
    "OVERNIGHT_SHEET = \"DATA\"\n",
    "OVERNIGHT_AUDIENCE_COL = 'Audience' # Assumed to be in absolute numbers\n",
    "\n",
    "BSR_FILE = \"data/WF 3 F1-R12 - Great Britain.xlsx\" \n",
    "BSR_SHEET = \"Worksheet\"\n",
    "BSR_HEADER_ROW = 5 \n",
    "# â­ BSR TARGET COLUMN DEFINITION â­\n",
    "BSR_TARGET_COL_RAW = 'Aud Metered (000s) 3+' # This column is in thousands\n",
    "\n",
    "# â­ FINAL MERGE KEY COLUMNS (4-PART KEY) â­\n",
    "COUNTRY_COLUMN = 'Country' \n",
    "DATE_COLUMN = 'Date'\n",
    "CHANNEL_COLUMN = 'Channel'\n",
    "SESSION_COMPETITION_COLUMN = 'Session_Competition'\n",
    "FINAL_MERGE_ON_COLS = [COUNTRY_COLUMN, CHANNEL_COLUMN, DATE_COLUMN, SESSION_COMPETITION_COLUMN]\n",
    "\n",
    "# â­ FILTER & SWAP CONSTANTS â­\n",
    "GP_FILTER_COL = 'Grand Prix'\n",
    "GP_FILTER_VALUE = '15_Dutch GP'\n",
    "\n",
    "DATE_SWAP_RULES = {\n",
    "    pd.to_datetime('2025-08-30'): pd.to_datetime('2025-07-05'),\n",
    "    pd.to_datetime('2025-08-31'): pd.to_datetime('2025-07-06'),\n",
    "    pd.to_datetime('2025-07-06'): pd.to_datetime('2025-07-06') \n",
    "}\n",
    "\n",
    "# Overnight Columns needed\n",
    "OVERNIGHT_KEY_COLS = [COUNTRY_COLUMN, CHANNEL_COLUMN, DATE_COLUMN, 'Session', GP_FILTER_COL, OVERNIGHT_AUDIENCE_COL] \n",
    "\n",
    "# BSR Column Mapping\n",
    "BSR_COLUMN_MAPPING = {\n",
    "    'TV-Channel': CHANNEL_COLUMN, \n",
    "    'Market': COUNTRY_COLUMN,\n",
    "    'Date': DATE_COLUMN,\n",
    "    'Competition': SESSION_COMPETITION_COLUMN, \n",
    "    BSR_TARGET_COL_RAW: BSR_TARGET_COL_RAW # Map BSR audience column to itself\n",
    "}\n",
    "\n",
    "\n",
    "# --- Function to Load and Standardize Data ---\n",
    "def load_and_standardize_data(file_path, sheet_name, columns_map, header=None, use_cols=None):\n",
    "    \"\"\"Loads data, renames columns according to map, and standardizes key columns.\"\"\"\n",
    "    try:\n",
    "        cols_to_load = use_cols if use_cols is not None else list(columns_map.keys())\n",
    "        \n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols=cols_to_load, \n",
    "            header=header\n",
    "        )\n",
    "        \n",
    "        df = df.rename(columns=columns_map)\n",
    "        \n",
    "        # Standardize String Columns\n",
    "        for col in [COUNTRY_COLUMN, CHANNEL_COLUMN, SESSION_COMPETITION_COLUMN, 'Session']:\n",
    "            if col in df.columns:\n",
    "                 df[col] = df[col].astype(str).str.strip().str.upper()\n",
    "\n",
    "        # Standardize Filter Column\n",
    "        if GP_FILTER_COL in df.columns:\n",
    "            df[GP_FILTER_COL] = df[GP_FILTER_COL].astype(str).str.strip()\n",
    "        \n",
    "        # Convert Date column to datetime\n",
    "        if DATE_COLUMN in df.columns:\n",
    "             df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN], errors='coerce')\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred during loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN LOGIC ---\n",
    "print(\"--- Starting Final Audience Update Process ---\")\n",
    "\n",
    "# 1. Load Overnight data \n",
    "df_overnight = load_and_standardize_data(\n",
    "    OVERNIGHT_FILE, \n",
    "    OVERNIGHT_SHEET, \n",
    "    {col: col for col in OVERNIGHT_KEY_COLS}, \n",
    "    header=0,\n",
    "    use_cols=list({col: col for col in OVERNIGHT_KEY_COLS}.keys())\n",
    ")\n",
    "if df_overnight is None:\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# â­ STEP A: APPLY DATE SWAP LOGIC â­\n",
    "if DATE_COLUMN in df_overnight.columns:\n",
    "    for original_date, target_date in DATE_SWAP_RULES.items():\n",
    "        if isinstance(original_date, pd.Timestamp):\n",
    "             date_mask = df_overnight[DATE_COLUMN] == original_date\n",
    "             df_overnight.loc[date_mask, DATE_COLUMN] = target_date\n",
    "    print(f\"âœ… Applied date transformation: {len(DATE_SWAP_RULES)} rules processed.\")\n",
    "\n",
    "\n",
    "# â­ STEP B: APPLY THE GRAND PRIX FILTER â­\n",
    "if GP_FILTER_COL in df_overnight.columns:\n",
    "    df_overnight = df_overnight[df_overnight[GP_FILTER_COL] == GP_FILTER_VALUE].copy()\n",
    "    print(f\"âœ… Filtered Overnight data to only '{GP_FILTER_VALUE}'. Rows: {len(df_overnight)}\")\n",
    "\n",
    "\n",
    "# â­ STEP C: FORCE SESSION ALIGNMENT â­\n",
    "TARGET_DATE_QUALIFYING = pd.to_datetime('2025-07-05')\n",
    "TARGET_DATE_RACE = pd.to_datetime('2025-07-06')\n",
    "SESSION_COL_NAME = 'Session'\n",
    "\n",
    "if SESSION_COL_NAME in df_overnight.columns and DATE_COLUMN in df_overnight.columns:\n",
    "    df_overnight.loc[df_overnight[DATE_COLUMN] == TARGET_DATE_QUALIFYING, SESSION_COL_NAME] = 'QUALIFYING'\n",
    "    df_overnight.loc[df_overnight[DATE_COLUMN] == TARGET_DATE_RACE, SESSION_COL_NAME] = 'RACE'\n",
    "    print(\"âœ… Forced Overnight 'Session' alignment for QUALIFYING and RACE dates.\")\n",
    "\n",
    "# â­ STEP D: Map Overnight 'Session' to 'Session_Competition' and convert Audience to numeric â­\n",
    "df_overnight = df_overnight.rename(columns={'Session': SESSION_COMPETITION_COLUMN})\n",
    "df_overnight[OVERNIGHT_AUDIENCE_COL] = pd.to_numeric(df_overnight[OVERNIGHT_AUDIENCE_COL], errors='coerce')\n",
    "\n",
    "\n",
    "# 2. Load BSR data \n",
    "df_bsr = load_and_standardize_data(\n",
    "    BSR_FILE, \n",
    "    BSR_SHEET, \n",
    "    BSR_COLUMN_MAPPING, \n",
    "    header=BSR_HEADER_ROW,\n",
    "    use_cols=None \n",
    ")\n",
    "if df_bsr is None:\n",
    "    raise SystemExit(1)\n",
    "df_bsr[BSR_TARGET_COL_RAW] = pd.to_numeric(df_bsr[BSR_TARGET_COL_RAW], errors='coerce')\n",
    "\n",
    "print(f\"Total rows in BSR file: {len(df_bsr)}\")\n",
    "\n",
    "# --- 3. AGGREGATE OVERNIGHT DATA ---\n",
    "df_overnight_max = df_overnight.groupby(FINAL_MERGE_ON_COLS, dropna=False)[OVERNIGHT_AUDIENCE_COL].max().reset_index()\n",
    "df_overnight_max = df_overnight_max.rename(columns={OVERNIGHT_AUDIENCE_COL: 'Max_Overnight_Audience'})\n",
    "\n",
    "# --- 4. MERGE AND COMPARE ---\n",
    "\n",
    "# Merge the BSR file with the simplified Overnight max audience data\n",
    "merged_df = df_bsr.merge(\n",
    "    df_overnight_max, \n",
    "    on=FINAL_MERGE_ON_COLS, \n",
    "    how='left' # Use a left join to keep all BSR rows\n",
    ")\n",
    "\n",
    "# Scale BSR audience to absolute numbers (multiplying by 1000) FOR COMPARISON ONLY\n",
    "temp_bsr_abs = merged_df[BSR_TARGET_COL_RAW] * 1000.0\n",
    "\n",
    "# 5. Create a boolean mask for the conditional update\n",
    "# Comparison is (Max_Overnight_Audience in absolute) > (temp_bsr_abs in absolute)\n",
    "update_mask = (merged_df['Max_Overnight_Audience'] > temp_bsr_abs) & \\\n",
    "              (merged_df[BSR_TARGET_COL_RAW].notna()) & \\\n",
    "              (merged_df['Max_Overnight_Audience'].notna())\n",
    "\n",
    "# 6. Perform the update\n",
    "rows_updated = update_mask.sum()\n",
    "\n",
    "# Update the target column in the merged DataFrame\n",
    "# Scale the absolute Overnight Audience back to thousands before writing it to the BSR column\n",
    "updated_value_in_thousands = merged_df.loc[update_mask, 'Max_Overnight_Audience'] / 1000.0\n",
    "merged_df.loc[update_mask, BSR_TARGET_COL_RAW] = updated_value_in_thousands\n",
    "\n",
    "# Copy the updated column back to the original BSR DataFrame\n",
    "df_bsr[BSR_TARGET_COL_RAW] = merged_df[BSR_TARGET_COL_RAW]\n",
    "\n",
    "\n",
    "# --- FINAL OUTPUT ---\n",
    "print(\"\\n--- Update Summary ---\")\n",
    "print(f\"Total rows in final BSR DataFrame: {len(df_bsr)}\")\n",
    "print(f\"Total BSR rows updated (where Max Overnight > BSR): {rows_updated}\")\n",
    "\n",
    "# 7. Save the updated BSR to a new file \n",
    "OUTPUT_FILE = \"data/Updated_BSR_Report_Final_v2.xlsx\"\n",
    "df_bsr.to_excel(OUTPUT_FILE, sheet_name=BSR_SHEET, index=False)\n",
    "print(f\"\\nâœ… Updated BSR data saved to: **{OUTPUT_FILE}**\")\n",
    "\n",
    "if rows_updated > 0:\n",
    "    print(\"\\n--- Sample of Updated Rows (BSR) ---\")\n",
    "    output_cols = FINAL_MERGE_ON_COLS + [BSR_TARGET_COL_RAW] \n",
    "    cols_to_display = [col for col in output_cols if col in df_bsr.columns]\n",
    "    # Ensure date column is formatted for display\n",
    "    df_sample = df_bsr.loc[update_mask.index[update_mask], cols_to_display].copy()\n",
    "    if DATE_COLUMN in df_sample.columns:\n",
    "        df_sample[DATE_COLUMN] = df_sample[DATE_COLUMN].dt.strftime('%Y-%m-%d')\n",
    "    print(df_sample.head().to_string(index=False))\n",
    "else:\n",
    "    print(\"No BSR audience values were updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e693ff-d33e-4bd6-bd05-c710eaa2fcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
